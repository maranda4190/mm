#!/usr/bin/env python3
"""
AIæŠ•èµ„æ–°é—»ç›‘æ§ç³»ç»Ÿæµ‹è¯•è„šæœ¬
"""

import asyncio
import sys
import os
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

async def test_news_fetcher():
    """æµ‹è¯•æ–°é—»æŠ“å–åŠŸèƒ½"""
    print("ğŸ” æµ‹è¯•æ–°é—»æŠ“å–æ¨¡å—...")
    
    try:
        from scraper import NewsFetcher
        
        async with NewsFetcher() as fetcher:
            news_list = await fetcher.fetch_all_news()
        
        print(f"âœ… æˆåŠŸè·å– {len(news_list)} æ¡æ–°é—»")
        
        if news_list:
            sample_news = news_list[0]
            print(f"ğŸ“° ç¤ºä¾‹æ–°é—»æ ‡é¢˜: {sample_news.get('title', '')[:100]}...")
            print(f"ğŸ“… å‘å¸ƒæ—¶é—´: {sample_news.get('published_date', '')}")
            print(f"ğŸ”— æ¥æº: {sample_news.get('source', '')}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ–°é—»æŠ“å–æµ‹è¯•å¤±è´¥: {e}")
        return False

async def test_news_analyzer():
    """æµ‹è¯•æ–°é—»åˆ†æåŠŸèƒ½"""
    print("\nğŸ¤– æµ‹è¯•æ–°é—»åˆ†ææ¨¡å—...")
    
    try:
        from analyzer import NewsAnalyzer
        
        # åˆ›å»ºæµ‹è¯•æ–°é—»
        test_news = {
            'title': 'OpenAI raises $100 million in Series C funding led by Microsoft',
            'summary': 'Artificial intelligence company OpenAI announced today that it has raised $100 million in Series C funding round led by Microsoft Ventures.',
            'content': 'OpenAI, the artificial intelligence research company, has successfully raised $100 million in a Series C funding round. The round was led by Microsoft Ventures with participation from other major investors. This funding will be used to accelerate AI research and development.',
            'source': 'TechCrunch',
            'published_date': datetime.utcnow(),
            'link': 'https://example.com/openai-funding'
        }
        
        analyzer = NewsAnalyzer()
        analyzed_news = await analyzer.analyze_single_news(test_news)
        
        analysis = analyzed_news.get('analysis', {})
        print(f"âœ… åˆ†æå®Œæˆ")
        print(f"ğŸ“Š ç›¸å…³æ€§å¾—åˆ†: {analysis.get('relevance_score', 0):.2f}")
        print(f"â­ é‡è¦æ€§å¾—åˆ†: {analysis.get('importance_score', 0):.2f}")
        print(f"ğŸ·ï¸  åˆ†ç±»: {analysis.get('category', 'unknown')}")
        print(f"ğŸ’° èèµ„é‡‘é¢: {analysis.get('funding_amount', 'N/A')}")
        print(f"ğŸ”„ èèµ„è½®æ¬¡: {analysis.get('funding_round', 'N/A')}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ–°é—»åˆ†ææµ‹è¯•å¤±è´¥: {e}")
        return False

async def test_database():
    """æµ‹è¯•æ•°æ®åº“åŠŸèƒ½"""
    print("\nğŸ’¾ æµ‹è¯•æ•°æ®åº“æ¨¡å—...")
    
    try:
        from database import init_database, AsyncSessionLocal, NewsArticle
        from sqlalchemy import select
        
        # åˆå§‹åŒ–æ•°æ®åº“
        await init_database()
        
        # æµ‹è¯•æ•°æ®åº“è¿æ¥
        async with AsyncSessionLocal() as db:
            result = await db.execute(select(NewsArticle).limit(1))
            existing_news = result.scalar_one_or_none()
            
        print("âœ… æ•°æ®åº“è¿æ¥æ­£å¸¸")
        print(f"ğŸ“Š ç°æœ‰æ–°é—»æ•°é‡: {1 if existing_news else 0} (æµ‹è¯•)")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®åº“æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_config():
    """æµ‹è¯•é…ç½®"""
    print("\nâš™ï¸ æµ‹è¯•é…ç½®...")
    
    try:
        from config.settings import settings
        
        print(f"âœ… é…ç½®åŠ è½½æˆåŠŸ")
        print(f"ğŸ“Š æ–°é—»æºæ•°é‡: {len(settings.NEWS_SOURCES)}")
        print(f"â° æ›´æ–°é—´éš”: {settings.NEWS_UPDATE_INTERVAL} åˆ†é’Ÿ")
        print(f"ğŸ”‘ OpenAI API: {'å·²é…ç½®' if settings.OPENAI_API_KEY else 'æœªé…ç½®'}")
        print(f"ğŸ’¾ æ•°æ®åº“: {settings.DATABASE_URL}")
        
        if not settings.OPENAI_API_KEY:
            print("âš ï¸  è­¦å‘Šï¼šæœªé…ç½®OpenAI APIå¯†é’¥ï¼ŒAIåˆ†æåŠŸèƒ½å°†å—é™")
        
        return True
        
    except Exception as e:
        print(f"âŒ é…ç½®æµ‹è¯•å¤±è´¥: {e}")
        return False

async def run_integration_test():
    """è¿è¡Œå®Œæ•´çš„é›†æˆæµ‹è¯•"""
    print("\nğŸ§ª è¿è¡Œé›†æˆæµ‹è¯•...")
    
    try:
        from scraper import NewsFetcher
        from analyzer import NewsAnalyzer
        from database import init_database, AsyncSessionLocal, NewsArticle
        
        # åˆå§‹åŒ–æ•°æ®åº“
        await init_database()
        
        # è·å–æ–°é—»
        async with NewsFetcher() as fetcher:
            news_list = await fetcher.fetch_all_news()
        
        if not news_list:
            print("âš ï¸  æœªè·å–åˆ°æ–°é—»ï¼Œå¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜æˆ–æ–°é—»æºæ— ç›¸å…³å†…å®¹")
            return True
        
        # åˆ†ææ–°é—»
        analyzer = NewsAnalyzer()
        analyzed_news = await analyzer.analyze_news_batch(news_list[:3])  # åªæµ‹è¯•å‰3æ¡
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        async with AsyncSessionLocal() as db:
            saved_count = 0
            for news_data in analyzed_news:
                try:
                    analysis = news_data.get('analysis', {})
                    article = NewsArticle(
                        title=news_data.get('title', ''),
                        link=news_data.get('link', ''),
                        summary=news_data.get('summary', ''),
                        content=news_data.get('content', ''),
                        published_date=news_data.get('published_date'),
                        source=news_data.get('source', ''),
                        analysis=analysis,
                        relevance_score=analysis.get('relevance_score', 0.0),
                        importance_score=analysis.get('importance_score', 0.0),
                        overall_score=analysis.get('overall_score', 0.0),
                        category=analysis.get('category', 'general'),
                        urgency=analysis.get('urgency', 'low'),
                        analyzed_at=datetime.utcnow(),
                        is_processed=True
                    )
                    
                    db.add(article)
                    saved_count += 1
                    
                except Exception as e:
                    print(f"è­¦å‘Šï¼šä¿å­˜æ–°é—»å¤±è´¥ - {e}")
                    continue
            
            await db.commit()
        
        print(f"âœ… é›†æˆæµ‹è¯•å®Œæˆï¼Œå¤„ç†äº† {saved_count} æ¡æ–°é—»")
        return True
        
    except Exception as e:
        print(f"âŒ é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        return False

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª AIæŠ•èµ„æ–°é—»ç›‘æ§ç³»ç»Ÿ - åŠŸèƒ½æµ‹è¯•")
    print("=" * 50)
    
    all_tests_passed = True
    
    # åŸºç¡€é…ç½®æµ‹è¯•
    if not test_config():
        all_tests_passed = False
    
    # æ•°æ®åº“æµ‹è¯•
    if not await test_database():
        all_tests_passed = False
    
    # æ–°é—»æŠ“å–æµ‹è¯•
    if not await test_news_fetcher():
        all_tests_passed = False
    
    # æ–°é—»åˆ†ææµ‹è¯•
    if not await test_news_analyzer():
        all_tests_passed = False
    
    # é›†æˆæµ‹è¯•
    if not await run_integration_test():
        all_tests_passed = False
    
    print("\n" + "=" * 50)
    if all_tests_passed:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿå¯ä»¥æ­£å¸¸è¿è¡Œ")
        print("ğŸ’¡ ä¸‹ä¸€æ­¥: è¿è¡Œ ./start.sh æˆ– python main.py å¯åŠ¨ç³»ç»Ÿ")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œä¾èµ–")
        print("ğŸ’¡ å°è¯•è¿è¡Œ: pip install -r requirements.txt")
    
    print("=" * 50)

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\næµ‹è¯•å·²ä¸­æ–­")
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        sys.exit(1)